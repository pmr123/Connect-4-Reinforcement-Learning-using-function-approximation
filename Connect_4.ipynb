{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import tkinter as tk\n",
    "from keras.models import Sequential\n",
    "from keras.models import load_model\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import save_model\n",
    "import random\n",
    "from collections import deque\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "import matplotlib.pylab as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unit = 40\n",
    "r = (unit-10)/2\n",
    "maze_h = 6\n",
    "maze_w = 7\n",
    "reward_w = 20 #wins\n",
    "reward_l = -20 #loss\n",
    "reward_draw = 10 #draw\n",
    "reward_illegal = -1000 #illegal move\n",
    "reward_e = -1 #otherwise. negative to reduce steps\n",
    "moves = []\n",
    "illegal_moves = []\n",
    "moves_avg = []\n",
    "illegal_moves_avg = []\n",
    "episode_count = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Board:\n",
    "    def __init__(self):\n",
    "        self.window = tk.Tk()\n",
    "        self.window.title(\"Connect 4\")\n",
    "        self.window.geometry('{0}x{1}'.format(maze_w*unit,(maze_h+1)*unit))\n",
    "        self.action_space = [_ for _ in range(maze_w)]\n",
    "        self.n_action = len(self.action_space)\n",
    "        self.state = np.zeros([maze_h,maze_w])\n",
    "        self.num_piece = 0\n",
    "        self.click_allowed = 1\n",
    "        self.player = 1\n",
    "        self.playstyle = 1 # 1 = pvp, 2 = pva \n",
    "        self.color = {1:\"red\", 2:\"yellow\"}\n",
    "        self.build_board()\n",
    "            \n",
    "    def build_board(self):\n",
    "        self.canvas = tk.Canvas(self.window,bg='black',width=maze_w*unit,height=(maze_h+1)*unit)\n",
    "        \n",
    "        self.top = self.canvas.create_rectangle(\n",
    "            0, 0,\n",
    "            maze_w*unit, unit,\n",
    "            fill=\"white\"\n",
    "        )\n",
    "        \n",
    "        self.mylabel = self.canvas.create_text((maze_w*unit/2), (unit/2), text=\"Player 1's turn\")\n",
    "        \n",
    "        for x in range(0,maze_w):\n",
    "            for y in range(0,maze_h):\n",
    "                cx = (x*unit) + (unit/2)\n",
    "                cy = (y*unit) + (unit/2) + unit\n",
    "                s = \"hole\"+str(x)+str(y)\n",
    "                self.s = self.canvas.create_oval(\n",
    "                    cx-r,cy-r,\n",
    "                    cx+r,cy+r,\n",
    "                    fill=\"white\"\n",
    "                )\n",
    "        \n",
    "        def button_clicked(event):\n",
    "            if(self.click_allowed == 1):\n",
    "                action = (event.x)//unit\n",
    "                s, r, d, wt = self.get_state_reward(self.player, action)\n",
    "                if d == True:\n",
    "                    if r == reward_w:\n",
    "                        self.click_allowed = 0\n",
    "                        self.canvas.itemconfigure(self.mylabel, text=\"Player \"+str(self.player)+\" wins\")\n",
    "                    else:\n",
    "                        self.canvas.itemconfigure(self.mylabel, text=\"Draw\")\n",
    "                else:\n",
    "                    if r == reward_e:\n",
    "                        if self.player == 1:\n",
    "                            self.player = 2\n",
    "                            self.canvas.itemconfigure(self.mylabel, text=\"Player \"+str(self.player)+\"\\'s turn\")\n",
    "                        else:\n",
    "                            self.player = 1\n",
    "                            self.canvas.itemconfigure(self.mylabel, text=\"Player \"+str(self.player)+\"\\'s turn\")\n",
    "                        if self.playstyle == 2:\n",
    "                            self.click_allowed = 0\n",
    "                \n",
    "\n",
    "        \n",
    "        self.canvas.bind(\"<Button-1>\", button_clicked)\n",
    "        \n",
    "        self.canvas.pack()\n",
    "                        \n",
    "    def render(self):\n",
    "        self.window.update()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.window.update()\n",
    "        self.num_piece = 0\n",
    "        self.canvas.delete(\"pieces\")\n",
    "        self.state = np.zeros([maze_h,maze_w])\n",
    "        self.player = 1\n",
    "        self.canvas.itemconfigure(self.mylabel, text=\"Player 1\\'s turn\")\n",
    "        self.render()\n",
    "        return self.state\n",
    "        \n",
    "    def get_state_reward(self, player, action):\n",
    "        y = maze_h-1\n",
    "        flag = 1 #to check if valid action is taken\n",
    "        while(y >= 0):\n",
    "            if(self.state[y][action] == 0):\n",
    "                self.state[y][action] = player\n",
    "                self.place_piece(action, y, player)\n",
    "                flag = 0\n",
    "                break\n",
    "            else:\n",
    "                y -= 1\n",
    "        \n",
    "        if self.num_piece == (maze_h*maze_w):\n",
    "            reward = reward_draw\n",
    "            done = True\n",
    "            return self.state, reward, done, 0 #win_type = 0\n",
    "        \n",
    "        if flag: #invalid action\n",
    "            reward, win_type = reward_illegal, 0 #win_type = 0\n",
    "        else: #vald action\n",
    "            reward, win_type = self.is_terminal(self.state,player)\n",
    "        \n",
    "        if reward == reward_w:\n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "        \n",
    "        return self.state, reward, done, win_type\n",
    "    \n",
    "    def place_piece(self, x, y, player):\n",
    "        cx = (x*unit) + (unit/2)\n",
    "        cy = (y*unit) + (unit/2) + unit\n",
    "        s = \"piece\"+str(x)+str(y)\n",
    "        self.s = self.canvas.create_oval(\n",
    "            cx-r,cy-r,\n",
    "            cx+r,cy+r,\n",
    "            fill=self.color[player],\n",
    "            tags=\"pieces\"\n",
    "        )\n",
    "        self.num_piece += 1\n",
    "\n",
    "    def is_terminal(self, board, player):\n",
    "        # Check horizontal locations for win\n",
    "        for c in range(maze_w-3):\n",
    "            for r in range(maze_h):\n",
    "                if board[r][c] == player and board[r][c+1] == player and board[r][c+2] == player and board[r][c+3] == player:\n",
    "                    return reward_w, 1\n",
    "\n",
    "        # Check vertical locations for win\n",
    "        for c in range(maze_w):\n",
    "            for r in range(maze_h-3):\n",
    "                if board[r][c] == player and board[r+1][c] == player and board[r+2][c] == player and board[r+3][c] == player:\n",
    "                    return reward_w, 2\n",
    "\n",
    "        # Check positively sloped diaganols\n",
    "        for c in range(maze_w-3):\n",
    "            for r in range(maze_h-3):\n",
    "                if board[r][c] == player and board[r+1][c+1] == player and board[r+2][c+2] == player and board[r+3][c+3] == player:\n",
    "                    return reward_w, 3\n",
    "\n",
    "        # Check negatively sloped diaganols\n",
    "        for c in range(maze_w-3):\n",
    "            for r in range(3, maze_h):\n",
    "                if board[r][c] == player and board[r-1][c+1] == player and board[r-2][c+2] == player and board[r-3][c+3] == player:\n",
    "                    return reward_w, 3     \n",
    "        \n",
    "        return reward_e, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, state_size, alpha=1, gamma=0.95, epsilon=1.0, epsilon_min=0.01, epsilon_decay=0.995, model_name=\"connect4.hdf5\"):\n",
    "        self.action_history = []\n",
    "        self.state_size = state_size\n",
    "        self.action_size = maze_w\n",
    "        self.reward = 0\n",
    "        self.memory1 = deque(maxlen=(maze_h*maze_w))\n",
    "        self.memory2 = deque(maxlen=(maze_h*maze_w))\n",
    "        self.model_name = model_name\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        try:\n",
    "            self.model = load_model(model_name)\n",
    "        except OSError:\n",
    "            self.model = self.create_model()\n",
    "    \n",
    "    def create_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(units=32, input_shape=(self.state_size, ), activation=\"relu\"))\n",
    "        model.add(Dense(units=8, activation=\"relu\"))\n",
    "        model.add(Dense(units=self.action_size, activation=\"linear\"))\n",
    "        model.compile(loss=\"mse\", optimizer=Adam())\n",
    "        return model\n",
    "    \n",
    "    def reset(self):\n",
    "        self.reward = 0\n",
    "        self.action_history = []\n",
    "        self.memory1 = deque(maxlen=(maze_h*maze_w))\n",
    "        self.memory2 = deque(maxlen=(maze_h*maze_w))\n",
    "    \n",
    "    def act(self, state):\n",
    "        state = np.reshape(state, (1, (maze_w*maze_h)+1))\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            action = random.randrange(self.action_size)\n",
    "        else:\n",
    "            actions = self.model.predict(state)\n",
    "            action = np.argmax(actions[0])\n",
    "        \n",
    "        self.action_history.append(action)\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def learn(self, state, action, reward, next_state, done):\n",
    "        state = np.reshape(state, (1, (maze_w*maze_h)+1))\n",
    "        next_state = np.reshape(next_state, (1, (maze_w*maze_h)+1))\n",
    "        current_q_value = self.model.predict(state)[0][action]\n",
    "        if done:\n",
    "            target = current_q_value + self.alpha*(reward)\n",
    "        else:\n",
    "            next_q_value = np.amax(self.model.predict(next_state)[0])\n",
    "            target = current_q_value + self.alpha*(reward + self.gamma*next_q_value - current_q_value)\n",
    "        predicted_target = self.model.predict(state)\n",
    "        predicted_target[0][action] = target\n",
    "        self.model.fit(state, predicted_target, epochs=10, verbose=0)\n",
    "    \n",
    "    def experience_replay(self):\n",
    "        for state, action, reward, next_state, done in self.memory1:\n",
    "            state = np.reshape(state, (1, (maze_w*maze_h)+1))\n",
    "            next_state = np.reshape(next_state, (1, (maze_w*maze_h)+1))\n",
    "            current_q_value = self.model.predict(state)[0][action]\n",
    "            if done:\n",
    "                target = current_q_value + self.alpha*(reward)\n",
    "            else:\n",
    "                next_q_value = np.amax(self.model.predict(next_state)[0])\n",
    "                target = current_q_value + self.alpha*(reward + self.gamma*next_q_value - current_q_value)\n",
    "            predicted_target = self.model.predict(state)\n",
    "            predicted_target[0][action] = target\n",
    "            self.model.fit(state, predicted_target, epochs=10, verbose=0)\n",
    "        for state, action, reward, next_state, done in self.memory2:\n",
    "            state = np.reshape(state, (1, (maze_w*maze_h)+1))\n",
    "            next_state = np.reshape(next_state, (1, (maze_w*maze_h)+1))\n",
    "            current_q_value = self.model.predict(state)[0][action]\n",
    "            if done:\n",
    "                target = current_q_value + self.alpha*(reward)\n",
    "            else:\n",
    "                next_q_value = np.amax(self.model.predict(next_state)[0])\n",
    "                target = current_q_value + self.alpha*(reward + self.gamma*next_q_value - current_q_value)\n",
    "            predicted_target = self.model.predict(state)\n",
    "            predicted_target[0][action] = target\n",
    "            self.model.fit(state, predicted_target, epochs=10, verbose=0)\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(choice):\n",
    "    board = Board()\n",
    "    if choice == 2:\n",
    "        agent = Agent((maze_h*maze_w)+1, epsilon=0.0)\n",
    "        board.click_allowed = 1\n",
    "        board.player = 1\n",
    "        board.playstyle = 2\n",
    "        episode_count = 1\n",
    "    elif choice == 3:\n",
    "        agent = Agent((maze_h*maze_w)+1)\n",
    "        board.click_allowed = 0\n",
    "        board.player = 1\n",
    "    \n",
    "    def plot_reward_movements():\n",
    "        fig, axs = plt.subplots(2, 2)\n",
    "        axs[0, 0].plot(range(1,episode_count+1), moves)\n",
    "        axs[0, 0].set_title('Moves')\n",
    "        axs[0, 1].plot(range(1,episode_count+1), moves_avg)\n",
    "        axs[0, 1].set_title('Average Moves')\n",
    "        axs[1, 0].plot(range(1,episode_count+1), illegal_moves)\n",
    "        axs[1, 0].set_title('Illegal Moves')\n",
    "        axs[1, 1].plot(range(1,episode_count+1), illegal_moves_avg)\n",
    "        axs[1, 1].set_title('Average Illegal Moves')\n",
    "        fig.show()\n",
    "        fig.tight_layout(pad=3.0)\n",
    "    \n",
    "    def run_experiment():\n",
    "        start_time = time.time()\n",
    "        w1 = 0\n",
    "        l1 = 0\n",
    "        w2 = 0\n",
    "        l2 = 0\n",
    "        d = 0\n",
    "        win_type = {1:0, 2:0, 3:0}\n",
    "        for e in range(1,episode_count+1):\n",
    "            print(\"Episode  {0}/{1}\".format(e, episode_count))\n",
    "            agent.reset()\n",
    "            state = board.reset()\n",
    "            state = state.flatten()\n",
    "            done = False\n",
    "            moves_count = 0\n",
    "            illegal_moves_count = 0\n",
    "\n",
    "            while True:\n",
    "                if ((choice == 3) or ((choice == 2) and (board.player == 2))):\n",
    "                    moves_count += 1\n",
    "                    mstate = np.append(state,board.player) #mstate = state with player appended\n",
    "                    action = agent.act(mstate)\n",
    "                    nstate, reward, done, wt = board.get_state_reward(board.player, action)\n",
    "                    nstate = nstate.flatten()\n",
    "                    mnstate = np.append(nstate,board.player) #mnstate = next state with player appended\n",
    "                    if board.player == 1:\n",
    "                        agent.memory1.append((mstate, action, reward, mnstate, done))\n",
    "                    elif board.player == 2:\n",
    "                        agent.memory2.append((mstate, action, reward, mnstate, done))\n",
    "                    agent.learn(mstate, action, reward, mnstate, done)\n",
    "\n",
    "                    if done == True:\n",
    "                        if choice == 3:\n",
    "                            if reward == reward_w:\n",
    "                                board.canvas.itemconfigure(board.mylabel, text=\"Player \"+str(board.player)+\" wins\")\n",
    "                                print(\"Player \"+str(board.player)+\" wins\")\n",
    "                                win_type[wt] += 1\n",
    "                                if board.player == 1: #update loss of other player\n",
    "                                    agent.memory2.append((np.append(state,2), action, reward_l, np.append(nstate,2), done))\n",
    "                                    w1 += 1\n",
    "                                    l2 += 1\n",
    "                                elif board.player == 2:\n",
    "                                    agent.memory1.append((np.append(state,1), action, reward_l, np.append(nstate,1), done))\n",
    "                                    w2 += 1\n",
    "                                    l1 += 1\n",
    "\n",
    "                            elif reward == reward_draw:\n",
    "                                board.canvas.itemconfigure(board.mylabel, text=\"Draw\")\n",
    "                                print(\"Draw\")\n",
    "                                d += 1\n",
    "                                if board.player == 1: #update draw of other player\n",
    "                                    agent.memory2.append((np.append(state,2), action, reward_draw, np.append(nstate,2), done))\n",
    "                                elif board.player == 2:\n",
    "                                    agent.memory1.append((np.append(state,1), action, reward_draw, np.append(nstate,1), done))\n",
    "                            board.render()\n",
    "                            print(\"illegal moves \",illegal_moves_count)\n",
    "                            print(\"moves \",moves_count)\n",
    "                            agent.experience_replay()\n",
    "                            time.sleep(1)\n",
    "                        break\n",
    "                    else:\n",
    "                        if reward == reward_illegal:\n",
    "                            illegal_moves_count += 1\n",
    "                        else: #switch player \n",
    "                            if board.player == 1:\n",
    "                                board.player = 2\n",
    "                                board.canvas.itemconfigure(board.mylabel, text=\"Player \"+str(board.player)+\"\\'s turn\")\n",
    "                            else:\n",
    "                                board.player = 1\n",
    "                                board.canvas.itemconfigure(board.mylabel, text=\"Player \"+str(board.player)+\"\\'s turn\")\n",
    "                        if board.playstyle == 2:\n",
    "                            board.click_allowed = 1\n",
    "                        board.render()\n",
    "\n",
    "                    state = nstate\n",
    "            \n",
    "            moves.append(moves_count)\n",
    "            illegal_moves.append(illegal_moves_count)\n",
    "            if e == 1:\n",
    "                moves_avg.append(moves_count)\n",
    "                illegal_moves_avg.append(illegal_moves_count)\n",
    "            else:\n",
    "                moves_avg.append(((moves_avg[-1]*len(moves_avg))+moves_count)/(len(moves_avg)+1))\n",
    "                illegal_moves_avg.append(((illegal_moves_avg[-1]*len(illegal_moves_avg))+illegal_moves_count)/(len(illegal_moves_avg)+1))\n",
    "\n",
    "        if choice == 3:\n",
    "            end_time = time.time()\n",
    "            print(\"Training time = {0} sec\".format(end_time-start_time))\n",
    "            print(\"Total Episodes - \",episode_count)\n",
    "            print(\"Agent 1\")\n",
    "            print(\"Wins - {0},Losses - {1},Draws -{2}\".format(w1,l1,d))\n",
    "            print(\"Agent 2\")\n",
    "            print(\"Wins - {0},Losses - {1},Draws -{2}\".format(w2,l2,d))\n",
    "            print(\"Win Types\")\n",
    "            print(\"Horizontal - \",win_type[1])\n",
    "            print(\"Vertical - \",win_type[2])\n",
    "            print(\"Diagonal - \",win_type[3])\n",
    "            save_model(agent.model,'connect4.hdf5')\n",
    "            plot_reward_movements()\n",
    "        \n",
    "    \n",
    "    board.window.after(10, run_experiment)\n",
    "    board.window.mainloop()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__==\"__main__\":\n",
    "    choice = 0\n",
    "    while(not(choice>=1 and choice<=3)):\n",
    "        print(\"Enter play style\")\n",
    "        print(\"1) Player vs Player\")\n",
    "        print(\"2) Player vs AI Agent (Evaluation)\")\n",
    "        print(\"3) AI Agent vs AI Agent (Training)\")\n",
    "        choice = int(input())\n",
    "        if(not(choice>=1 and choice<=3)):\n",
    "            print(\"Wrong choice entered! Please enter again\")\n",
    "    if choice == 1:\n",
    "        env = Board()\n",
    "        env.window.mainloop()\n",
    "    else:\n",
    "        main(choice)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
